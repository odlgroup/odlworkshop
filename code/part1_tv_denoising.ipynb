{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising with total variation using the PDHG method\n",
    "\n",
    "In this notebook we denoise an image using total variation (TV) regularization and a nonnegativity constraint. We solve the minimization problem using the PDHG method.\n",
    "\n",
    "## Contents\n",
    "\n",
    "### Mathematics\n",
    "- Problem formulation, including minimal basics on convex optimization\n",
    "- The PDHG method\n",
    "- Rewriting the problem in a form suitable for PDHG\n",
    "  - Introducing auxiliary variables\n",
    "  - Product spaces and operators\n",
    "\n",
    "### Implementation\n",
    "- Defining a space of discretized functions\n",
    "- Generating a phantom and its noisy version from an image\n",
    "- Defining a product space operator\n",
    "- Setting up functionals\n",
    "- Choosing method parameters\n",
    "- Running the solver\n",
    "- Checking the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "We aim for solving the TV denoising problem\n",
    "\n",
    "$$\n",
    "    \\min_{x \\geq 0} \\left[ \\| x - y \\|_2^2 + \\alpha \\| \\nabla x \\|_1 \\right]\n",
    "$$\n",
    "\n",
    "for given data $y$ and some regularization parameter $\\alpha > 0$. For now we do not exactly specify over which sets we optimize.\n",
    "\n",
    "This problem is **convex** since both terms are convex, but **not differentiable** due to the 1-norm - thus, methods for smooth optimization will not work well.\n",
    "\n",
    "Another issue with this kind of minimization problem is the presence of a linear operator $\\nabla$ in the non-differentiable term. Many methods for optimizing the sum of convex functions need the **proximal operator**\n",
    "\n",
    "$$\n",
    "    \\mathrm{prox}_f(x) = \\mathrm{arg}\\min_y \\left[ f(y) + \\frac{1}{2} \\| x - y \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "of all terms available in an efficiently usable way, i.e., in closed form or as an easy-to-solve optimization problem. This is, in general, not the case for $f(x) = \\| \\nabla x \\|$ due to the composition with the gradient operator. The proximal of the (scaled) 1-norm, on the other hand, is well-known as\n",
    "\n",
    "$$\n",
    "    \\mathrm{prox}_{\\sigma \\| \\cdot \\|_1}(x) = S_\\sigma(x),\n",
    "$$\n",
    "\n",
    "where $S_\\sigma$ is the pointwise *soft-shrinkage* operator\n",
    "\n",
    "$$\n",
    "    S_\\sigma(x)(t) = \n",
    "    \\begin{cases}\n",
    "        x(t) + \\sigma, &\\text{if } x(t) < -\\sigma,     \\\\\n",
    "        0,             &\\text{if } \\left|x(t)\\right| \\leq \\sigma, \\\\\n",
    "        x(t) - \\sigma, &\\text{if } x(t) > \\sigma.      \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Here is a visualization of the soft-shrinkage operation for $\\sigma = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-3, 3, 31)\n",
    "\n",
    "def soft_shrink(x, sigma):\n",
    "    res = np.zeros_like(x)\n",
    "    res[x < -sigma] = x[x < -sigma] + sigma\n",
    "    res[x > sigma] = x[x > sigma] - sigma\n",
    "    return res\n",
    "\n",
    "\n",
    "plt.plot(x, x, label='x')\n",
    "plt.plot(x, soft_shrink(x, 1), label='Soft-shrinkage of x')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PDHG method\n",
    "\n",
    "The *primal-dual hybrid gradient* (PDHG) method, also known as \"Chambolle-Pock method\" [CP2011] is an optimization method to solve problems of the form\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X} \\left[ f(L x) + g(x) \\right]\n",
    "$$\n",
    "\n",
    "with convex functions $f$ and $g$, and a linear operator $L$. The optimum is determined in a *Hilbert space* $X$, and we have\n",
    "\n",
    "$$\n",
    "    L : X \\to Y,          \\\\\n",
    "    f : Y \\to \\mathbb{R}, \\\\\n",
    "    g : X \\to \\mathbb{R}\n",
    "$$\n",
    "\n",
    "with some other Hilbert space $Y$. If you are not familiar with Hilbert spaces and linear operators, think $X = \\mathbb{R}^n$, $Y = \\mathbb{R}^m$ and $L \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "The crucial advantage of this method over, e.g., ISTA [BT2009] is that it requires the proximals of $f$ and $g$, but **not** of the composition $f \\circ L$.\n",
    "\n",
    "## Rewriting the original problem\n",
    "\n",
    "We now rewrite our original denoising problem in a form that is suitable for the PDHG method. There is, of course, no unique way of doing this, but we can use the method requirements as guidelines.\n",
    "\n",
    "In particular, the proximal operators of $f$ and $g$ should be known, which means\n",
    "\n",
    "- no composition with an operator (except for a few special cases not relevant here) and\n",
    "- no sums.\n",
    "\n",
    "We also specify the spaces now since one of the subsequent steps will require it. For convenience, we choose the **square-integrable functions** on (a subset of) $\\mathbb{R}^d$, $X = L^2(\\mathbb{R}^d)$. The gradient of a function $x \\in X$ will be a $d$-stack of functions $\\partial_i x \\in X$, written as $\\nabla x \\in X^d$.\n",
    "\n",
    "---\n",
    "**Note:**\n",
    "\n",
    "If you are not acquainted with these notions, think of $X = \\mathbb{R}^n$. In this case, instead of function values $x(t)$ for $t \\in \\mathbb{R}^d$, you get discrete values $x_i,\\ i=1, \\dots, n$. The (discrete) gradient of an element $x \\in X$ will then be a $(d \\times n)$ matrix, $D x \\in \\mathbb{R}^{d \\times n} \\equiv (\\mathbb{R}^n)^d$.\n",
    "\n",
    "---\n",
    "\n",
    "### The positivity constraint\n",
    "\n",
    "In the original formulation, we optimize over all *pointwise nonnegative* $x$, but since that set is not a vector space, we have to use the **indicator function**\n",
    "\n",
    "$$\n",
    "    \\iota_0(x)(t) =\n",
    "    \\begin{cases}\n",
    "        0,      &\\text{if } x(t) >= 0, \\\\\n",
    "        \\infty, &\\text{if } x(t) < 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "to rewrite the problem as a minimization over a vector space:\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X, x \\geq 0} f(x) \\quad \\Leftrightarrow \\quad \\min_{x \\in X} \\left[ f(x) + \\iota_0(x) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "**Note:**\n",
    "\n",
    "Sometimes we \"abuse\" notation by switching between writing, e.g., $\\iota_0(x(t))$ and $\\iota_0(x)(t)$ equivalently. \n",
    "\n",
    "- The first variant uses $\\iota_0$ as a function $[-\\infty, \\infty] \\to [-\\infty, \\infty]$ that takes in a real value $x(t)$ and produces a real value $\\iota_0(x(t))$.\n",
    "- The second variant takes the function $x$ as a whole and maps it to a new function $\\iota_0(x)$ which evaluates to $\\iota_0(x(t))$ at $t$. In that sense, $\\iota_0$ is a mapping $X \\to X$ for this case.\n",
    "\n",
    "---\n",
    "\n",
    "Revisiting our original problem formulation\n",
    "\n",
    "$$\n",
    "    \\min_{x \\geq 0} \\left[ \\| x - y \\|_2^2 + \\alpha \\| \\nabla x \\|_1 \\right]\n",
    "$$\n",
    "\n",
    "and rewriting the positivity constraint now yields\n",
    "\n",
    "$$\n",
    "    \\min_x \\left[ \\| x - y \\|_2^2 + \\alpha \\| \\nabla x \\|_1 + \\iota_0(x) \\right].\n",
    "$$\n",
    "\n",
    "### Splitting into $f$ and $g$\n",
    "\n",
    "We now need to identify the $f$ and $g$ parts for the PDHG method. Remember that $f$ is allowed to be composed with linear operators, $g$ not. We will choose to take $g = \\iota_0$ and put the other two terms into $f$ since later on, when we consider more general problems with a forward operator (e.g. tomography), the term $\\| x - y \\|_2^2$ will also contain a linear operator, $\\| A x - y \\|_2^2$.\n",
    "\n",
    "### Decoupling the $f$ terms\n",
    "\n",
    "Now we have a problem: According to the used splitting, we would get $f(x) = \\| x - y \\|_2^2 + \\alpha \\| \\nabla x \\|_1$. However, since we need $\\mathrm{prox}_{\\sigma f}$, we *cannot use sums of functions in $f$*.\n",
    "\n",
    "Fortunately, we can use a trick to solve this issue. The computation of the functional sum can be viewed like this:\n",
    "\n",
    "$$\n",
    "    x \\to \\binom{z_1 = x}{z_2 = \\nabla x} \\to \\| z_1 - y \\|_2^2 + \\alpha \\| z_2 \\|_1,\n",
    "$$\n",
    "\n",
    "i.e., by using auxiliary variables $z_1$ and $z_2$ the problem can be split into independent parts. Note that the variable $z_2$ is the gradient of an element in $X$, hence it lies in $X^d$ as explained [above](TV_denoising_with_PDHG.ipynb#Rewriting-the-original-problem).\n",
    "\n",
    "The following procedure leads to the desired result:\n",
    "\n",
    "1. Define a \"stacked\" vector $z = \\binom{z_1}{z_2}$ with $z_1 \\in X, z_2 \\in X^d$. In other words, work in the (Cartesian) **product space** $Z = X \\times X^d$. For the special case $X = \\mathbb{R}^n$, this is equivalent to stacking an $\\mathbb{R}^n$ vector and an $\\mathbb{R}^{d \\times n}$ matrix either as an $\\mathbb{R}^{(d+1) \\times n}$ matrix or an $\\mathbb{R}^{(d+1) n}$ vector.\n",
    "\n",
    "2. Define $f$ as a function on the product space $Z$ rather than on the space $X$, namely\n",
    "\n",
    "   $$\n",
    "       f(z) = \\| z_1 - y \\|_2^2 + \\alpha \\| z_2 \\|_1 = f_1(z_1) + f_2(z_2).\n",
    "   $$\n",
    "   \n",
    "   This sum is **separable**, meaning that there are no mixed terms, and such a separable sum has the nice property that the proximal separates along the components:\n",
    "   \n",
    "   $$\n",
    "       \\mathrm{prox}_f(z) = \\binom{\\mathrm{prox}_{f_1}(z_1)}{\\mathrm{prox}_{f_2}(z_2)}.\n",
    "   $$\n",
    "   \n",
    "   These proximals have closed form expressions.\n",
    "   \n",
    "3. Define the linear operator $L: X \\to Z$ as the **broadcasting** of the identity operator $I: X \\to X$ and the gradient operator $\\nabla: X \\to X^d$:\n",
    "\n",
    "   $$\n",
    "       L = \\binom{I}{\\nabla}, \\quad L x = \\binom{x}{\\nabla x} \\in Z.\n",
    "   $$\n",
    "   \n",
    "Now it is easy to see that indeed,\n",
    "\n",
    "$$\n",
    "    f(L x) = f\\binom{x}{\\nabla x} = f_1(x) + f_2(\\nabla x) = \\|x - y\\|_2^2 + \\alpha\\|\\nabla x\\|_1.\n",
    "$$\n",
    "\n",
    "---\n",
    "**Note:**\n",
    "This trick of introducing auxiliary variables can always be applied when sums of functions should be optimized. However, one should be aware that this procedure comes at a cost: In a numerical implementation, each variable must be stored in memory during optimization. In our case, this means that the memory usage is at least $(d+1)$ times the size of a vector $x \\in X$. For large-scale 3D problems, this can be a serious issue.\n",
    "\n",
    "---\n",
    "   \n",
    "### Final problem formulation\n",
    "\n",
    "We can now put everything together and state the problem in a form suitable for the PDHG method:\n",
    "\n",
    "- Reconstruction space $X = L^2(\\mathbb{R}^d)$\n",
    "- Product space $Z = X \\times X^d$\n",
    "- Forward operator\n",
    "  $$\n",
    "      L: X \\to Z,\\quad L = \\binom{I}{\\nabla}\n",
    "  $$\n",
    "- Function $f$ that will be composed with $L$:  \n",
    "  $$\n",
    "      f: Z \\to \\mathbb{R},\\quad f(z) = \\| z_1 - y \\|_2^2 + \\alpha \\| z_2 \\|_1 \n",
    "  $$\n",
    "- Function $g$ without linear operator:\n",
    "  $$\n",
    "      g: X \\to \\mathbb{R},\\quad g = \\iota_0\n",
    "  $$\n",
    "  \n",
    "Now we finally turn to the implementation in ODL. This is derived from the [PDHG denoising example in ODL](https://github.com/odlgroup/odl/blob/master/examples/solvers/pdhg_denoising.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will use the [\"ascent\" test image from SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.ascent.html) with additive white noise (since that is what $\\|\\cdot - y\\|_2^2$ is suitable for)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "\n",
    "image = scipy.misc.ascent().astype('float32')\n",
    "plt.imshow(image, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the reconstruction space $X$. In ODL, function spaces consist of functions defined on cuboids $Q = [a, b]$, where $a, b \\in \\mathbb{R}^d$ with $a \\leq b$ entry-wise.\n",
    "For discretization, the number $N \\in \\mathbb{N}^d$ of points per axis needs to be specified. This is called the *shape* of the space (as for NumPy arrays). \n",
    "\n",
    "Now, a continuous function $x$ is represented by its discrete values $x_i = x(t_i)$, where\n",
    "\n",
    "$$\n",
    "    t_i = t_0 + i \\Delta t,\\quad 0 \\leq i < N,\\ t_0, \\Delta t \\in \\mathbb{R}^d,\n",
    "$$\n",
    "\n",
    "are uniform samples from the cuboid $[a, b]$ (note that addition and multiplication above are componentwise).\n",
    "\n",
    "In two dimensions, for instance, this is a way of interpreting an image $x \\in \\mathbb{R}^{N_1 \\times N_2}$ as a discretized function on some rectangle $[a_1, b_1] \\times [a_2, b_2]$.\n",
    "For our example, we choose $a = 0$ and $b = N$ to make each pixel have size $1 \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import odl\n",
    "\n",
    "# The shape we need to use is the transposed of the original shape since\n",
    "# ODL uses 'xy' axes while the image is stored with 'ij' axis convention.\n",
    "shape = image.T.shape\n",
    "\n",
    "# `min_pt` corresponds to `a`, `max_pt` to `b`\n",
    "X = odl.uniform_discr(min_pt=[0, 0], max_pt=shape, shape=shape)\n",
    "\n",
    "print('Pixel size:', X.cell_sides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we make the (rescaled) image an element $x_{\\mathrm{true}} \\in X$ and define the noisy image $y = x_{\\mathrm{true}} + 0.1 \\cdot N(0, 1)$ as a space element, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rotation converts from 'ij' to 'xy' axes\n",
    "image /= image.max()\n",
    "x_true = X.element(np.rot90(image, -1))\n",
    "# To get predictable randomness, we explicitly seed the random number generator\n",
    "with odl.util.NumpyRandomSeed(123):\n",
    "    y = x_true + 0.1 * odl.phantom.white_noise(X)\n",
    "\n",
    "# We assign to a dummy variable since the function returns a `Figure` object\n",
    "# that would be displayed immediately, leading to double plotting\n",
    "_ = x_true.show(title='Original image (x_true)')\n",
    "_ = y.show(title='Noisy image (y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the parts of the problem:\n",
    "\n",
    "- Forward operator\n",
    "  $$\n",
    "      L: X \\to Z,\\quad L = \\binom{I}{\\nabla}\n",
    "  $$\n",
    "\n",
    "Note that we don't have to set up the product space $Z$ explicitly since the `odl.Gradient` operator does it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = odl.Gradient(X)\n",
    "print('Gradient domain X:', grad.domain)\n",
    "print('Gradient range X^d:', grad.range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = odl.IdentityOperator(X)\n",
    "L = odl.BroadcastOperator(I, grad)\n",
    "print('Domain of L:', L.domain)\n",
    "print('Range of L:', L.range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function $f$ that will be composed with $L$:  \n",
    "  $$\n",
    "      f: Z \\to \\mathbb{R},\\quad f(z) = \\| z_1 - y \\|_2^2 + \\alpha \\| z_2 \\|_1 = f_1(z_1) + f_2(z_2)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `.translated(y)` takes care of the `. - y` part in the function\n",
    "f_1 = odl.solvers.L2NormSquared(X).translated(y)\n",
    "# The regularization parameter `alpha` is multiplied with the L1 norm.\n",
    "# The L1 norm must be defined on X^d, the range of the gradient.\n",
    "alpha = 0.15\n",
    "f_2 = alpha * odl.solvers.L1Norm(grad.range)\n",
    "f = odl.solvers.SeparableSum(f_1, f_2)\n",
    "\n",
    "# We can test whether everything makes sense by evaluating `f(L(x))`\n",
    "# at some arbitrary `x` in `X`. It should produce a scalar.\n",
    "print(f(L(X.zero())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function $g$ without linear operator:\n",
    "  $$\n",
    "      g: X \\to \\mathbb{R},\\quad g = \\iota_0\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = odl.solvers.IndicatorNonnegativity(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can invoke the solver with the problem components as defined above. To guarantee convergence, the step-size-like parameters $\\tau$ (primal step) and $\\sigma$ (dual step) need to be chosen according to the criterion\n",
    "\n",
    "$$\n",
    "    \\|L\\|^2 \\tau \\sigma < 1\n",
    "$$\n",
    "\n",
    "(For more information you can type `?? odl.solvers.pdhg` in a code cell.)\n",
    "\n",
    "Here, $\\|L\\|$ is the *operator norm* of $L$, like the Frobenius norm of a matrix. Usually it's safe to estimate the norm, add a bit of safety margin and then choose\n",
    "\n",
    "$$\n",
    "    \\tau = \\sigma = \\frac{1}{\\|L\\|}.\n",
    "$$\n",
    "\n",
    "Fortunately, we have a tool to estimate this norm. It uses the [Power Method](https://en.wikipedia.org/wiki/Power_iteration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot start the iteration from a constant vector since the\n",
    "# gradient would produce 0, which is invalid in the power iteration.\n",
    "# The noisy image `y` should do.\n",
    "L_norm = 1.1 * odl.power_method_opnorm(L, xstart=y, maxiter=20)\n",
    "\n",
    "tau = 1.0 / L_norm\n",
    "sigma = 1.0 / L_norm\n",
    "\n",
    "print('||L|| =', L_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point\n",
    "x = X.zero()\n",
    "\n",
    "# Run PDHG method. The vector `x` is updated in-place.\n",
    "odl.solvers.pdhg(x, f, g, L, tau=tau, sigma=sigma, niter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visually investigate the result, or we can apply quality measures (Figures of Merit, FOMs) from the `odl.contrib.fom` subpackage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = x_true.show('True image')\n",
    "_ = y.show('Noisy image')\n",
    "_ = x.show('Denoised image')\n",
    "_ = (x_true - x).show('Difference true - denoised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odl.contrib import fom\n",
    "\n",
    "print('Noisy')\n",
    "print('-----')\n",
    "print('Mean squared error:', fom.mean_squared_error(y, x_true))\n",
    "print('PSNR:', fom.psnr(y, x_true))\n",
    "print('SSIM:', fom.ssim(y, x_true))\n",
    "print('')\n",
    "\n",
    "print('Denoised')\n",
    "print('--------')\n",
    "print('Mean squared error:', fom.mean_squared_error(x, x_true))\n",
    "print('PSNR:', fom.psnr(x, x_true))\n",
    "print('SSIM:', fom.ssim(x, x_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[BT2009] Beck, A and Teboulle, M. *A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems*. SIAM Journal on Imaging Sciences, Vol. 2, No. 1 (2009), pp. 183-202.\n",
    "\n",
    "\n",
    "[CP2011] Chambolle, A and Pock, T. *A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging*. Journal of Mathematical Imaging and Vision, 40 (2011), pp 120-145.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
