{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification\n",
    "\n",
    "In this notebook we tackle the perhaps most well known problem in all of machine learning, classifying hand-written digits.\n",
    "\n",
    "The particular dataset we will use is the MNIST (Modified National Institute of Standards and Technology)\n",
    "The digits are 28x28 pixel images that look somewhat like this:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2202312/32365318-b0ccc44a-c079-11e7-8fb1-6b1566c0bdc4.png)\n",
    "\n",
    "Each digit has been hand classified, e.g. for the above 9-7-0-9-0-...\n",
    "\n",
    "Our task is to teach a machine to perform this classification, i.e. we want to find a function $\\mathcal{T}_\\theta$ such that\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|$\\mathcal{T}_\\theta$(|<img align=\"center\" src=\"https://user-images.githubusercontent.com/2202312/33177374-b134e572-d062-11e7-87c7-0574c6f5bee9.png\" width=\"28\"/>|) = 4|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "This should run without errors if all dependencies are installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "In these exercies we'll use the software library [tensorflow](https://www.tensorflow.org/), which is a state of the art library for numerical computations in python developed by Google. The package supports computation on both the CPU and GPU, and also supports automatic differentiation (more on that later).\n",
    "\n",
    "### Execution\n",
    "Writing code in tensorflow is quite similar to classical python, except for one thing: tensorflow uses *deferred execution*, this means that instead of executing the code in one step (called eager execution), the computational graph is first defined, and it is then fed data upon which the computation is performed. Users from compiled languages will recongnize this execution method.\n",
    "\n",
    "The execution flow typically goes like this:\n",
    "\n",
    "* Define placeholders for the input\n",
    "* Define what computations should be performed\n",
    "* Feed the placeholders data\n",
    "\n",
    "This computational model has several upsides that are sadly not evident in simple samples like these, including parallelization, packaging and optimization.\n",
    "\n",
    "### Name scopes\n",
    "Tensorflow also uses scopes (e.g. [`name_scope`](https://www.tensorflow.org/api_docs/python/tf/name_scope)) to organize the code. This can interact very nicely with external tools like [tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard), but is also used as comments in the code and to give better error messages.\n",
    "\n",
    "### Batched computation\n",
    "Finally, tensorflow (and most other deep learning libraries) uses batched execution. This means that instead of computing the result one element at a time, or one image at a time, the result is computed simultaneously for a *batch* of images, e.g. several. The batch size is typically in the range of 1 to 512, depending on the input size. This is done in order to enable faster computations on modern hardware. Due to this, the shape of objects used with tensorflow is typically\n",
    "\n",
    "```\n",
    "[BATCH_SIZE, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start a tensorflow session\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Set the random seed to enable reproducible code\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data and utilities\n",
    "\n",
    "We now need to get the data we will use, which in this case is the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a set of digits 70000 hand-written digits, of which 60000 are used for training and 10000 for testing.\n",
    "\n",
    "In addition to this, we create a utility `evaluate(...)` that we will use to evaluate how good the classification is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data\n",
    "\n",
    "In machine learning, the first step is *always* to try to understand your data. How does it look? What pre-processing has been done?\n",
    "\n",
    "In this case we can [read the documentation](http://yann.lecun.com/exdb/mnist/) to find:\n",
    "\n",
    "* The images were rescaled to size 20 x 20\n",
    "* The numbers are all fit in a 28 x 28 square and centered according to center of mass $\\approx [14, 14]$\n",
    "* the values span exactly $[0, 1]$.\n",
    "* The numbers are from $\\approx 250$ writers and scrambled in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center of mass = (14.383709839842643, 14.340846509159318)\n",
      "min value = 0.0, max value = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUv8Hf5v6lCN/uH4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dae1483198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.test.images[0].reshape(28, 28)\n",
    "value =  mnist.test.labels[0]\n",
    "print('center of mass = {}'.format(scipy.ndimage.measurements.center_of_mass(img)))\n",
    "print('min value = {}, max value = {}'.format(np.min(img), np.max(img)))\n",
    "plt.imshow(img, cmap='Greys_r');\n",
    "plt.title('digit = {}'.format(value))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Before we start defining methods, we must first settle on a definition of what it meas for a method to be good. Since our task is classification we simply use the $%$ of images correctly classified.\n",
    "\n",
    "In order to not cheat, we train on one set (conveniently called the \"train\" set) and evaluate on another, called the \"test\"-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = mnist.test.next_batch(10000)\n",
    "test_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "test_labels = batch[1]\n",
    "\n",
    "def evaluate(result_tensor, data_placeholder):\n",
    "    \"\"\"Evaluate a reconstruction method by computing the % correct.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_tensor : `tf.Tensor`, shape (None,)\n",
    "        The tensorflow tensor containing the result of the classification.\n",
    "    data_placeholder : `tf.Tensor`, shape (None, 28, 28, 1) or (None, 784)\n",
    "        The tensorflow tensor containing the input to the classification operator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : float\n",
    "        Mean squared error of the reconstruction.\n",
    "    \"\"\"\n",
    "    feed_images = np.reshape(test_images, [-1, *data_placeholder.shape[1:]])\n",
    "    result = result_tensor.eval(\n",
    "        feed_dict={data_placeholder: feed_images})\n",
    "\n",
    "    return np.mean(result == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would like to learn a function $f_\\theta$ such that $f(x) = n$ where $x$ is the image and $n$ the corresponding digit. The best such function would be given by minimization of\n",
    "$$\n",
    "    \\min_\\theta L(\\theta) \\quad \\text{where } L(\\theta) = \\frac{1}{N} \\sum_{i = 1}^N \\#\\{f_\\theta(x_i) \\neq n_i \\}\n",
    "$$\n",
    "However, this method is basically impossible or at least very hard to minimize since we cannot compute\n",
    "$$\n",
    "    \\frac{\\partial L(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "A solution to this is [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), perhaps the most well known and widely applied classification method. In this method we use the *probabilistic* estimator:\n",
    "$$\n",
    "p_{x}(n) = \\frac{\\exp(\\langle w_i, x \\rangle + b_i)}{\\sum_{j=0}^9 \\exp(\\langle w_j, x \\rangle + b_j)}\n",
    "$$\n",
    "where $w_i \\in \\mathbb{R}^{28 \\times 28}$ and $b_i \\in \\mathbb{R}$.\n",
    "\n",
    "The loss function is the mean of the negative logarithm of the probability $p_x(n)$ \n",
    "\\\\[\n",
    "- \\frac{1}{N} \\sum_{i = 1}^N \\log(p_{x_i}(n_i))\n",
    "\\\\]\n",
    "For computational reasons this is most easily computed as follows:\n",
    "\\\\[\n",
    "-\\frac{1}{N}\\sum_{x=1}^N\\sum_{i=0}^9 (n_i = n) \\log(p_{x_i}(n))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to actually compute $n_i = n$, we use the one-hot-encoding, which can be seen as computing the true probabilities of each digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toh = tf.one_hot([1, 1, 2], depth=3)\n",
    "toh.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Implementation\n",
    "\n",
    "We start with an elementary implementation in `TensorFlow`. Here there are a few components we'll use:\n",
    "\n",
    "* [`tf.placeholder`](https://www.tensorflow.org/api_docs/python/tf/placeholder) is as it sounds a placeholder for some value we will later provide. Here we use $shape=(None, 784)$ where $None$ indicates that this value is not a-priori determined, we can use any number. $784$ is simply $28 \\times 28$.\n",
    "* [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) is something (in fact the only thing) that can change its value over time. This will be there parameters $\\theta$ that we minimize over. We also provide initial values, normally distributed for the weights and zero for the bias.\n",
    "\n",
    "We then compute the logarithmic probabilities according to the formulae above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('elementary_network'):\n",
    "    # Create a placeholder for our input data (no computation is done here)\n",
    "    inp = tf.placeholder(shape=(None, 784), dtype=tf.float32, name=\"input\")\n",
    "    \n",
    "    # Create the parameters (weight, bias) of the model\n",
    "    weights = tf.Variable(tf.random_normal((784, 10)), name=\"weights\")\n",
    "    bias = tf.Variable(tf.zeros((10)), name=\"bias\")\n",
    "    \n",
    "    # Compute the probabilities (this is all lazy, no computations are actually performed)\n",
    "    lin = tf.matmul(inp, weights) + bias\n",
    "    elin = tf.exp(lin)\n",
    "    Z = tf.reduce_sum(elin, axis=1, keep_dims=True)\n",
    "    prob = elin / Z\n",
    "    log_prob = tf.log(prob)\n",
    "    \n",
    "    # Prediction by the most likely digit\n",
    "    pred = tf.argmax(log_prob, axis=1)  # axis 0 is the batch, axis 1 is the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function which measures how good our parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_loss\"):\n",
    "    labels = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "    determ = tf.one_hot(labels, depth=10)\n",
    "    loss = -tf.reduce_mean(determ*log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "We'll train the network using gradient descent, i.e.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\omega \\nabla L(\\theta)$$\n",
    "\n",
    "where $\\omega$ is the *learning rate*, or step size.\n",
    "\n",
    "Note that in machine learning, we typically use *stochastic* gradient descent (SGD). In these methods we don't use all of the data to compute the gradient, only a small subset called a mini-batch. Here we use 128 images in each training step.\n",
    "\n",
    "Further, while for this case computing the gradient would be quite simple, once we move to harder and more complicated models doing so would be basically impossible to do by hand. To work around this, all major deep learning frameworks implement [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). This may sound fancy, but automatic differentiation is simply the chain rule for the adjoint of the derivative derivative:\n",
    "\n",
    "$$\n",
    "[D(g \\circ f)(x)]^*(y) = [Df(x)]^*\\big([Dg(f(x))]^*(y)\\bigr)\n",
    "$$\n",
    "\n",
    "Tensorflow implements it using the [`tf.gradients`](https://www.tensorflow.org/api_docs/python/tf/gradients) command.\n",
    "\n",
    "As noted above, `tf.Variable`s are the only mutable objects in tensorflow and we'll use that to update them in place using the `var.assign` method. Note that we still do not actually perform this operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_training\"):\n",
    "    learning_rate = .1\n",
    "\n",
    "    variables = [weights, bias]\n",
    "    gradients = tf.gradients(loss, variables)\n",
    "    update_ops = [var.assign(var - learning_rate*grad) \n",
    "                  for var, grad in zip(variables, gradients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the code above was lazy, nothing has actually happened. Before we start we need to initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network by feeding data from the training set and occationally evalute the performance on our test set, this is the first point we actually start doing computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8%, 43.2%, 60.5%, 67.7%, 71.5%, 74.0%, 76.0%, 77.3%, 78.5%, 79.1%, 80.0%, 80.6%, 81.2%, 81.6%, 82.0%, 82.5%, 83.0%, 83.3%, 83.6%, 83.8%, 84.0%, 84.3%, 84.5%, 84.6%, 84.8%, 85.0%, 85.2%, 85.4%, 85.5%, 85.7%, 85.7%, 85.8%, 86.0%, 86.1%, 86.2%, 86.3%, 86.6%, 86.7%, 86.8%, 86.9%, 87.0%, 87.1%, 87.1%, 87.2%, 87.2%, 87.3%, 87.4%, 87.5%, 87.5%, 87.6%, 87.8%, 87.8%, 87.8%, 87.9%, 87.9%, 88.0%, 88.0%, 88.1%, 88.1%, 88.2%, 88.2%, 88.2%, 88.2%, 88.3%, 88.2%, 88.3%, 88.4%, 88.4%, 88.4%, 88.5%, 88.5%, 88.5%, 88.5%, 88.6%, 88.7%, 88.7%, 88.8%, 88.7%, 88.8%, 88.8%, 88.9%, 88.9%, 88.8%, 88.9%, 88.9%, 88.9%, 89.0%, 88.9%, 88.9%, 88.9%, 88.9%, 89.0%, 88.9%, 88.9%, 89.1%, 89.0%, 89.0%, 89.0%, 89.1%, 89.2%, "
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(update_ops, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow libraries\n",
    "\n",
    "While the above code solves our problem, it involved several small and perhaps obscure steps. Once we start moving to more complicated neural networks the code would become very repetetive.\n",
    "\n",
    "Since all of the steps are standardized, we can (and should) instead use built in tensorflow functions, this example does that, and all following examples will do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "The \"network\" can be computed using the [`tf.contrib.layers.fully_connected`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected) function which computes\n",
    "\n",
    "$$\\rho(Wx + b)$$\n",
    "\n",
    "where $\\rho$ is the activation function, $W$ is a matrix called the weights and $b$ is a real vector called the bias. Note that here we never explicitly construct these, they are hidden inside tensorflow.\n",
    "\n",
    "Also remember that above we needed to give an initializer to the weights and biases, this is now done automatically (but can be modified via the `weights_initializer` and `bias_initializer` parameters. However in this case we'll just use defaults. If we check the defaults, this means\n",
    "\n",
    "* `weights_initializer=`[`tf.contrib.layers.xavier_initializer`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer). This is a rather smart initializer, which basically choses the weights such that the norm of the input and output is approximately equal, which gives more stable training.\n",
    "* `bias_initializer=`[`tf.zeros_initializer`](https://www.tensorflow.org/api_docs/python/tf/zeros_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    logits = tf.contrib.layers.fully_connected(inp, \n",
    "                                               num_outputs=10,      # We only need to specify the number of outputs\n",
    "                                               activation_fn=None)  # Dont use any activation (include in loss instead)\n",
    "    pred = tf.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimization\n",
    "\n",
    "The loss function defined above should be done using the [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) function, which not only is easier to use, it is also more numerically stable.\n",
    "\n",
    "In addition to this, we don't really have to write our own optimizer, and there are in fact several very good optimizers built into tensorflow. In this example we'll use [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) which is a very popular and efficient gradient-based optimizer. All of the details regarding weights etc are hidden inside tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "Training the network looks about the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4%, 91.5%, 92.0%, 92.3%, 92.6%, 92.7%, 92.6%, 92.8%, 92.8%, 92.8%, "
     ]
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "The first \"deep\" neural networks were [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron). A multilayer perceptron , in these we have a function of the following form\n",
    "\n",
    "$$\n",
    "W_3\\rho(W_2\\rho(W_1 x + b_1) + b_2) + b_3\n",
    "$$\n",
    "\n",
    "Where $W_i \\in \\mathbb{R}^{n_{i - 1} \\times n_{i}}$ are matrices and $b_i \\in \\mathbb{R}^{n_{i}}$ vectors and $\\rho$ a pointwise nonlinearity $\\mathbb{R}^{n_{i}} \\to \\mathbb{R}^{n_{i}}$.\n",
    "\n",
    "In this very simple neural network we'll use the following parameters:\n",
    "\n",
    "* Network size:\n",
    "  * `n_0=784` ($28 \\times 28$)\n",
    "  * `n_1=128`\n",
    "  * `n_2=32`\n",
    "  * `n_3=10` (number of labels)\n",
    "* ReLU nonlinearities $\\rho$\n",
    "$$\n",
    "ReLU(x) = \\max(x, 0)\n",
    "$$\n",
    "\n",
    "\n",
    "Note that the logistic regression can be cast into this form (how?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2%, 96.3%, 97.3%, 97.2%, 97.8%, 97.5%, 97.6%, 97.9%, 97.7%, 97.8%, "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    x = tf.contrib.layers.fully_connected(inp, num_outputs=128)  # the default activation function is ReLU\n",
    "    x = tf.contrib.layers.fully_connected(x, num_outputs=32)\n",
    "    logits = tf.contrib.layers.fully_connected(x, \n",
    "                                               num_outputs=10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network\n",
    "\n",
    "Convolutional neural networks are a corner-stone of the deep learning revolution. Here instead of using traditionall fully-connected layers which connect each point with all other points, we use spatial convolutions instead. By doing this, we get a translation invariant operator that acts locally. \n",
    "\n",
    "A convolution operator $C : \\mathcal{X}^n \\to \\mathcal{X}^m$, where $\\mathcal{X}$ is some set of images and $n$ is called the number of channels, is defined by\n",
    "$$\n",
    "[C(x)]_i = \\sum_j (w_{ij} \\ast x_j + b_j)\n",
    "$$\n",
    "where the *weight* $w_{ij}$ is a convolution kernel and $b_j \\in \\mathbb{R}$ is the bias.\n",
    "\n",
    "In applications, there number of duplicate entries $n, m$ is typically $\\in [32, 512]$ and the convolution kernel is given by a small $e.g. 3 \\times 3$ stencil. We'll use the built in function [`tf.contrib.layers.conv2d`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d) in order to perform the convolutions efficiently. Specifically this function operates on arrays with shape `[batch, height, width, channels]`, which is called BHWC order. Do note that this is not universal and e.g. PyTorch uses BCHW order.\n",
    "\n",
    "In order to get non-local behaviour we stack several of these on top of each other in the same manner as a MLP.\n",
    "\n",
    "The following code is a very simplified convolutional neural network for digit classification with the following layers\n",
    "\n",
    "* `3x3` convolution with 32 channels and stride 2 and ReLU nonlinearity. Stride two means that we compute the convolution for every other pixel which means that the output will be half the size of the input.\n",
    "* `3x3` convolution with 32 channels and stride 2 and ReLU nonlinearity.\n",
    "* Flatten the images and channels to a one-dimensional array\n",
    "* $1568 \\to 128$ fully connected layer with ReLU nonlinearity\n",
    "* $128 \\to 10$ fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.5%, 98.1%, 98.6%, 98.6%, 98.7%, 98.6%, 98.5%, 98.8%, 98.7%, 98.7%, "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('convolutional_network'):\n",
    "    # Reshape the input from [?, 784] to [?, 28, 28, 1] where 1 is the number of channels (e.g. n above)\n",
    "    images = tf.reshape(inp, [-1, 28, 28, 1])\n",
    "    \n",
    "    x = tf.contrib.layers.conv2d(images, \n",
    "                                 num_outputs=32, # Number of \"channels\", e.g. duplicates of the image\n",
    "                                 kernel_size=3,  # size of the convolution kernel\n",
    "                                 stride=2)       # Use strides (jumps) to decrease the image size in each step\n",
    "    # x.shape = [?, 14, 14, 32]\n",
    "    x = tf.contrib.layers.conv2d(x, num_outputs=32, kernel_size=3, stride=2)\n",
    "    # x.shape = [?, 7, 7, 32]\n",
    "    x = tf.contrib.layers.flatten(x)    \n",
    "    # x.shape = [?, 1568]\n",
    "    \n",
    "    # It is typically a good idea to finish with fully connected layers \n",
    "    # in order to encode the non-translation invariant information\n",
    "    x = tf.contrib.layers.fully_connected(x, 128)\n",
    "    logits = tf.contrib.layers.fully_connected(x, 10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "From this notebook there are some take-aways\n",
    "\n",
    "* A classification problem involves taking an image and assigning it to a class (e.g. 4)\n",
    "* Several methods from machine learning can be used for classificication\n",
    "* Software libraries like tensorflow are great\n",
    "* Deeper neural networks work very well\n",
    "* Convolutions allow us to encode spatial information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
